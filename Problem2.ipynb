{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Step 1: Download the dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text  # This is the entire text data\n",
    "\n",
    "# Step 2: Prepare the dataset\n",
    "sequence_length = 20\n",
    "# Create a character mapping to integers\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encode the text into integers\n",
    "encoded_text = [char_to_int[ch] for ch in text]\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "def get_dataset(encoded_text, batch_size=128, sequence_length=20):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(0, len(encoded_text) - sequence_length):\n",
    "        seq = encoded_text[i:i+sequence_length]\n",
    "        target = encoded_text[i+sequence_length]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "\n",
    "    sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "    dataset = CharDataset(sequences, targets)\n",
    "\n",
    "    # Step 4: Create data loaders\n",
    "    batch_size = 128\n",
    "    train_size = int(len(dataset) * 0.8)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.5, model_type='lstm', fc_layer_num=1, fc_hidden_size=128):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc_layer_num = fc_layer_num\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        if model_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        elif model_type == 'gru':\n",
    "            self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        self.fc_input_layer = nn.Linear(hidden_size, fc_hidden_size)\n",
    "        self.fc_layers = nn.ModuleList([nn.Linear(fc_hidden_size, fc_hidden_size) for _ in range(fc_layer_num)])\n",
    "        self.fc_output_layer = nn.Linear(fc_hidden_size, output_size)\n",
    "        if dropout is not False:\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        output = output[:, -1, :]\n",
    "        output = self.fc_input_layer(output)\n",
    "        for fc_layer in self.fc_layers:\n",
    "            output = fc_layer(output)\n",
    "            output = torch.relu(output)\n",
    "            if self.dropout:\n",
    "                output = self.dropout(output)\n",
    "        output = self.fc_output_layer(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, epochs, learning_rate, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracys = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                output = model(X_batch)\n",
    "                loss = criterion(output, y_batch)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "            val_loss = val_loss / len(test_loader)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracy = correct / total\n",
    "            val_accuracys.append(val_accuracy)\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            print(f\"Epoch: {epoch+1}, Train Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}, Val Accuracy: {val_accuracy}\")\n",
    "    \n",
    "    return train_losses, val_losses, val_accuracys\n",
    "\n",
    "def predict_next_char(model, char_to_ix, ix_to_char, initial_str, max_length, device='cuda'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        initial_input = torch.tensor([char_to_ix[c] for c in initial_str[-max_length:]], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "        prediction = model(initial_input)\n",
    "        predicted_index = torch.argmax(prediction, dim=1).item()\n",
    "        return ix_to_char[predicted_index]\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def plot_losses_and_accuracy(train_losses, val_losses, val_accuracys):\n",
    "\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(val_accuracys, label='Val Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(hidden_size = 128,\n",
    "                epochs = 20,\n",
    "                batch_size = 64,\n",
    "                learning_rate = 0.003, \n",
    "                device= 'cuda',\n",
    "                chars = chars,\n",
    "                char_to_int = char_to_int,\n",
    "                int_to_char = int_to_char,\n",
    "                sequence_length = 20,\n",
    "                model_type = 'lstm',\n",
    "                train_times = 10,\n",
    "                if_dropout = 0.5,\n",
    "                fc_layer_num = 1,\n",
    "                fc_hidden_size = 128):\n",
    "\n",
    "    train_loader, test_loader = get_dataset(encoded_text, batch_size=batch_size, sequence_length=sequence_length)\n",
    "    time_1 = time.time()\n",
    "    for t in range(train_times):\n",
    "        # just for getting precise time\n",
    "        model = CharRNN(len(chars), hidden_size, len(chars), dropout=if_dropout, model_type=model_type, fc_layer_num=fc_layer_num, fc_hidden_size=fc_hidden_size)\n",
    "        train_losses, val_losses, val_accuracys = train(model, train_loader, test_loader, epochs, learning_rate, device)\n",
    "    time_2 = time.time()\n",
    "    \n",
    "\n",
    "\n",
    "    target_list =  test_loader.dataset[10][0].detach().cpu().numpy()\n",
    "    target_list = [int_to_char[i] for i in target_list]\n",
    "    target_y = test_loader.dataset[10][1].detach().cpu().numpy()\n",
    "    target_y = int_to_char[target_y.item()]\n",
    "    target_str = ''.join([x for x in target_list])\n",
    "\n",
    "    predicted_results = []\n",
    "    for i in range(50):\n",
    "        if predicted_results:\n",
    "            target_str = target_str + predicted_results[-1]\n",
    "        predicted_results.append(predict_next_char(model, char_to_int, int_to_char, target_str, sequence_length, device))\n",
    "    \n",
    "    print(\"==========================================resutls==========================================\")\n",
    "    print(f'model type: {model_type}, hidden size: {hidden_size}')\n",
    "    print(f'final training loss: {train_losses[-1]}, final validation loss: {val_losses[-1]}, final validation accuracy: {val_accuracys[-1]}')\n",
    "    print(f'results:{target_str}, label: {target_y}')\n",
    "    print(f'Average running time\" {(time_2 - time_1)/train_times}')\n",
    "    print(f'model complexity(number of parameters): {count_parameters(model)}')\n",
    "    print(\"===========================================================================================\")\n",
    "    return {'model_type': model_type, 'max_length':sequence_length, 'hidden_size': hidden_size, 'final validation accuracy': val_accuracys[-1], 'Average running time': (time_2 - time_1)/train_times, 'model complexity(number of parameters)': count_parameters(model)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 1.9241704254128504, Val Loss: 1.6752559433568843, Val Accuracy: 0.49378908438865854\n",
      "Epoch: 2, Train Loss: 1.7427712759784222, Val Loss: 1.614298998728198, Val Accuracy: 0.5119623445029698\n",
      "Epoch: 3, Train Loss: 1.6966177107989617, Val Loss: 1.5840329580859625, Val Accuracy: 0.518973439426202\n",
      "Epoch: 4, Train Loss: 1.671986974745869, Val Loss: 1.563457570743506, Val Accuracy: 0.5234652022862266\n",
      "Epoch: 5, Train Loss: 1.6547221989379965, Val Loss: 1.5537530172849208, Val Accuracy: 0.5293914602712092\n",
      "Epoch: 6, Train Loss: 1.646740502673049, Val Loss: 1.540959621597417, Val Accuracy: 0.5304269864395382\n",
      "Epoch: 7, Train Loss: 1.6382493785381043, Val Loss: 1.5489631322903394, Val Accuracy: 0.5306869886809369\n",
      "Epoch: 8, Train Loss: 1.6340533012264566, Val Loss: 1.5374829868304614, Val Accuracy: 0.5345152975456685\n",
      "Epoch: 9, Train Loss: 1.6320273638096474, Val Loss: 1.5420423599747632, Val Accuracy: 0.5322873473047182\n",
      "Epoch: 10, Train Loss: 1.628572565005965, Val Loss: 1.5287406139516038, Val Accuracy: 0.5370391124061414\n"
     ]
    }
   ],
   "source": [
    "get_results(hidden_size = 128,\n",
    "                epochs = 20,\n",
    "                batch_size = 128,\n",
    "                learning_rate = 0.003, \n",
    "                device= 'cuda',\n",
    "                chars = chars,\n",
    "                char_to_int = char_to_int,\n",
    "                int_to_char = int_to_char,\n",
    "                sequence_length = 20,\n",
    "                model_type = 'lstm',\n",
    "                train_times = 1,\n",
    "                if_dropout = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(hidden_size = 128,\n",
    "                epochs = 20,\n",
    "                batch_size = 128,\n",
    "                learning_rate = 0.003, \n",
    "                device= 'cuda',\n",
    "                chars = chars,\n",
    "                char_to_int = char_to_int,\n",
    "                int_to_char = int_to_char,\n",
    "                sequence_length = 30,\n",
    "                model_type = 'lstm',\n",
    "                train_times = 1,\n",
    "                if_dropout = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(hidden_size = 128,\n",
    "                epochs = 20,\n",
    "                batch_size = 128,\n",
    "                learning_rate = 0.003, \n",
    "                device= 'cuda',\n",
    "                chars = chars,\n",
    "                char_to_int = char_to_int,\n",
    "                int_to_char = int_to_char,\n",
    "                sequence_length = 20,\n",
    "                model_type = 'gru',\n",
    "                train_times = 1,\n",
    "                if_dropout = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(hidden_size = 128,\n",
    "                epochs = 20,\n",
    "                batch_size = 128,\n",
    "                learning_rate = 0.003, \n",
    "                device= 'cuda',\n",
    "                chars = chars,\n",
    "                char_to_int = char_to_int,\n",
    "                int_to_char = int_to_char,\n",
    "                sequence_length = 30,\n",
    "                model_type = 'gru',\n",
    "                train_times = 1,\n",
    "                if_dropout = 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
